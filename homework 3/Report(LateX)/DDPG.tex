\section{DDPG}\label{DDPG}

We construct DDPG using Tensorflow / Keras library, using similar code structure / ideas to PPO in section \ref{PPO}.

The DDPG model has 2 networks: actor and critic network, as well as 2 target networks actor target and critic target. 

We also use the target networks to optimize the model. In particular, at each step we first update the prediction and then minimize according to the MSE loss (pseudo-code):
\begin{lstlisting}
# generate critic preds
Qvals = critic.predict([next_state, action])
# generate actor-derived predictions for next action
next_actions = actor_target.predict([next_state])
# Q updates
next_Q = critic_target.predict([np.array(next_state_batch) , np.array(next_actions)])
Qprime = (reward_batch) + gamma * next_Q

# MSE loss
loss = mean_squared_error(Qprime, Qvals)
\end{lstlisting}

Here the ``batch'' refers to samples from the ``replay buffer'', or in other words from the already played out scenarios. 

While actor and critic networks are trained with back-propagation, the target networks are updated at each step according to (pseudo-code):
\begin{lstlisting}
params = tau * actor_model_params + (1-tau) * target_actor_model_params
params = tau * critic_model_params + (1-tau) * target_critic_model_params
\end{lstlisting}

We train the DDPG over 100 episodes, and let it play until ``done'' and for max. 1000 steps inside each episode.

In each episode, after experienced is collected, the actor and critic nets are updated and environment reset.

Early-stopping is introduced when the system starts scoring positive reward (goal).

Unfortunately, our model did not yield satisfying results despite training for a long time, and eventually we ran out of time before the submission deadline. We hope that the attached file (``DDPG'') explains our reasoning in how we designed the code.