{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "id": "O58KwRnhkYo6",
    "outputId": "58b9703e-a80d-4027-8d6d-b8ab15dbdedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease    \n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,376 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,205 kB]\n",
      "Fetched 2,833 kB in 4s (805 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsdl2-gfx-dev is already the newest version (1.0.4+dfsg-1).\n",
      "libsdl2-ttf-dev is already the newest version (2.0.14+dfsg1-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
      "fatal: destination path 'football' already exists and is not an empty directory.\n",
      "--2020-05-12 02:16:41--  https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.0.6.so\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 2404:6800:4008:c01::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45228864 (43M) [application/octet-stream]\n",
      "Saving to: ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’\n",
      "\n",
      "football/third_part 100%[===================>]  43.13M   107MB/s    in 0.4s    \n",
      "\n",
      "2020-05-12 02:16:41 (107 MB/s) - ‘football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so’ saved [45228864/45228864]\n",
      "\n",
      "Processing /content/football\n",
      "Requirement already satisfied: pygame==1.9.6 in /usr/local/lib/python3.6/dist-packages (from gfootball==2.0.6) (1.9.6)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gfootball==2.0.6) (4.1.2.30)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gfootball==2.0.6) (1.4.1)\n",
      "Requirement already satisfied: gym>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from gfootball==2.0.6) (0.17.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from gfootball==2.0.6) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python->gfootball==2.0.6) (1.18.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.11.0->gfootball==2.0.6) (1.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.11.0->gfootball==2.0.6) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.11.0->gfootball==2.0.6) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.11.0->gfootball==2.0.6) (0.16.0)\n",
      "Building wheels for collected packages: gfootball\n",
      "  Building wheel for gfootball (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gfootball: filename=gfootball-2.0.6-cp36-cp36m-linux_x86_64.whl size=38595160 sha256=c91a29b105f5408d37fae38b00e59316a9343cb224e08a13497bcbe81fea5b61\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c9n09_sq/wheels/41/ad/ae/8cf1d92b8694b10187e5daf33e8d5c248ffa5437e234ccbbee\n",
      "Successfully built gfootball\n",
      "Installing collected packages: gfootball\n",
      "  Found existing installation: gfootball 2.0.6\n",
      "    Uninstalling gfootball-2.0.6:\n",
      "      Successfully uninstalled gfootball-2.0.6\n",
      "Successfully installed gfootball-2.0.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup gfootball \n",
    "\"\"\"\n",
    "!apt-get update\n",
    "!apt-get install libsdl2-gfx-dev libsdl2-ttf-dev\n",
    "\n",
    "# Make sure that the Branch in git clone and in wget call matches !!\n",
    "!git clone -b v2.0.6 https://github.com/google-research/football.git\n",
    "!mkdir -p football/third_party/gfootball_engine/lib\n",
    "\n",
    "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.0.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
    "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOxSf4gzkkvT"
   },
   "outputs": [],
   "source": [
    "import gfootball.env as football_env\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten,concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6EtA7ColuBiw",
    "outputId": "0be2ebc0-2eb7-4a83-b09e-fe0ae555e4c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define env\n",
    "\"\"\"\n",
    "# define environment\n",
    "env = football_env.create_environment(env_name='academy_empty_goal', representation='simple115')\n",
    "\n",
    "# reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# define some variables\n",
    "state_dims = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVUd6FApwDZs"
   },
   "outputs": [],
   "source": [
    "def actor_model(input_dims, output_dims):\n",
    "  state_input = Input(shape=input_dims)\n",
    "  oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "  rewards = Input(shape=(1, 1,))\n",
    "  values = Input(shape=(1, 1,))\n",
    "\n",
    "  # F-C net (using Keras)\n",
    "  x = Dense(512, activation='relu', name='fc1')(state_input)\n",
    "  x = Dense(256, activation='relu', name='fc2')(x)\n",
    "  # use softmax to get out probabilities\n",
    "  out_actions = Dense(output_dims, activation='softmax', name='predictions')(x)\n",
    "\n",
    "  model = Model(inputs=[state_input, oldpolicy_probs],\n",
    "                  outputs=[out_actions])\n",
    "  model.compile(optimizer=Adam(lr=1e-4), loss='mse')\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def critic_model(input_dims,extra_dims):\n",
    "  # net for critic, loss here is just mse\n",
    "  state_input = Input(shape = input_dims)\n",
    "  extra_input = Input(shape = extra_dims)\n",
    "\n",
    "  # F-C net (using Keras)\n",
    "  x = Dense(int(np.array(input_dims))+int(np.array(extra_dims)), activation='relu', name='fc1')(state_input)\n",
    "  ax = Dense(512, activation='relu', name='fc2')(concatenate([x,extra_input]))\n",
    "  ax = Dense(512, activation='relu', name='fc3')(ax)\n",
    "  # final activation is tanh\n",
    "  out_actions = Dense(n_actions, activation='softmax')(ax)\n",
    "\n",
    "  model = Model(inputs=[state_input,extra_input], outputs=[out_actions])\n",
    "  model.compile(optimizer=Adam(lr=1e-4), loss='mse')\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSYJqnhWrYXU"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define networks\n",
    "\"\"\"\n",
    "# Actor\n",
    "actor = actor_model(input_dims = state_dims, output_dims = n_actions)\n",
    "actor_target = actor_model(input_dims = state_dims, output_dims = n_actions)\n",
    "\n",
    "# Critic\n",
    "critic = critic_model(input_dims = state_dims,extra_dims = (n_actions,))\n",
    "critic_target = critic_model(input_dims = state_dims,extra_dims = (n_actions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdgTsWsT2z35",
    "outputId": "328540f8-9877-420c-9ccc-5017ba9f1d96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115,)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\n",
    "\"\"\"\n",
    "ep_av_policyloss=[]\n",
    "ep_av_criticloss=[]\n",
    "rewards=[]\n",
    "steps_to_reward=[]\n",
    "\n",
    "\n",
    "import random\n",
    "#agent = DDPGagent(env)\n",
    "#noise = OUNoise(env.action_space)\n",
    "batch_size = 64\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "\n",
    "# episodes\n",
    "max_iters = 10\n",
    "\n",
    "# steps within episode\n",
    "max_steps = 300\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# outer loop\n",
    "dummy_n = np.zeros((1,1,n_actions))\n",
    "dummy_1 = np.zeros((1,1,1))\n",
    "steps=0\n",
    "for episode in range(max_iters):\n",
    "    print(\"episode {}...\".format(episode))  \n",
    " \n",
    "    env.reset()\n",
    "    state = env.reset()\n",
    "\n",
    "    agent_memory = []\n",
    "    \n",
    "    #noise.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "\n",
    "    buffer = deque(maxlen=0)\n",
    "    state_batch=[]\n",
    "    action_batch=[]\n",
    "    reward_batch=[]\n",
    "    next_state_batch=[]\n",
    "    done_batch=[]\n",
    "\n",
    "    target_reached = False\n",
    "    \n",
    "    state_rec=[]\n",
    "    action_rec=[]\n",
    "    reward_rec=[]\n",
    "    next_state_rec=[]\n",
    "    done_rec=[]\n",
    "    action_probs_rec = []\n",
    "    \n",
    "    episode_policy_loss=[]\n",
    "    episode_critic_loss=[]\n",
    "\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        \n",
    "        # forward pass for actor, with random initialization, to get the prob dist of actions (random_n and random_1 just used to fill the extra dimensions)\n",
    "        action_dist = actor.predict([state_input, dummy_n], steps=1)\n",
    "\n",
    "        # forward pass for critic, to get Q value\n",
    "        #q_value = model_critic.predict([state_input], steps=1)\n",
    "\n",
    "        # pick action index (sample for action distribuition)\n",
    "        action = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "        action_onehot = np.zeros(n_actions)\n",
    "        action_onehot[action] = 1\n",
    "        #break\n",
    "\n",
    "        \n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        \n",
    "\n",
    "        #agent_memory.append(state, action, reward, new_state, done)\n",
    "        state_rec.append(state)\n",
    "        action_rec.append(action)\n",
    "        reward_rec.append(reward)\n",
    "        next_state_rec.append(next_state)\n",
    "        done_rec.append(done)\n",
    "        action_probs_rec.append(action_dist)\n",
    "        memory = []\n",
    "\n",
    "        if done:\n",
    "          env.reset()\n",
    "\n",
    "        \n",
    "\n",
    "    # buffer\n",
    "        if len(reward_rec) > batch_size:\n",
    "          # sample a batch from record\n",
    "          state_batch = []\n",
    "          action_batch = []\n",
    "          reward_batch = []\n",
    "          next_state_batch = []\n",
    "          done_batch = []\n",
    "          probs_batch =[]\n",
    "\n",
    "          indices = random.sample(range(len(reward_rec)),batch_size)\n",
    "\n",
    "          for index in indices:\n",
    "            state_batch.append(state_rec[index])\n",
    "            action_batch.append(action_rec[index])\n",
    "            reward_batch.append(reward_rec[index])\n",
    "            next_state_batch.append(next_state_rec[index])\n",
    "            done_batch.append(done_rec[index])\n",
    "            probs_batch.append(action_probs_rec[index])\n",
    "\n",
    "    \n",
    "          # Critic loss     \n",
    "          action_batch_onehot = np.zeros((len(action_batch),n_actions))\n",
    "\n",
    "          for i in range(len(action_batch)):\n",
    "            action_batch_onehot[i,action_batch[i]] = 1   \n",
    "          \n",
    "          Qvals = critic.predict([np.array(next_state_batch), np.array(action_batch_onehot)])\n",
    "          \n",
    "          next_actions = actor_target.predict([np.array(next_state_batch),dummy_n], steps=1)\n",
    "          \n",
    "\n",
    "          next_Q = critic_target.predict([np.array(next_state_batch) , np.array(next_actions)])\n",
    "          \n",
    "          Qprime = np.reshape(np.array(reward_batch),(len(reward_batch),1)) + gamma * next_Q\n",
    "\n",
    "          \n",
    "          critic_loss = mean_squared_error(Qvals, Qprime)\n",
    "          \n",
    "          # Actor loss\n",
    "          policy_loss = -critic.predict([np.array(state_batch), np.array(actor.predict([state_batch, dummy_n], steps=1))])\n",
    "                                                                         \n",
    "          # update networks\n",
    "          print(\"episode: {}, step: {} / {}, fitting actor...\".format(episode,step, max_steps))\n",
    "          actor.fit([state_batch,probs_batch],[action_batch_onehot], verbose=True, shuffle=True, epochs=10)\n",
    "          print(\"episode: {}, step: {} / {}, fitting critic...\".format(episode,step, max_steps))\n",
    "          critic.fit([np.array(next_state_batch),np.array(action_batch_onehot)],[next_actions], verbose=True, shuffle=True, epochs=10)\n",
    "\n",
    "# update weights\n",
    "          actorW = actor.get_weights()\n",
    "          actor_tW = actor_target.get_weights()\n",
    "          criticW = critic.get_weights()\n",
    "          critic_tW = critic_target.get_weights()\n",
    "\n",
    "          for iter in range(len(actorW)):\n",
    "            actor_tW[iter] = tau * actorW[iter]+(1-tau)*actor_tW[iter]\n",
    "          for iter in range(len(criticW)):\n",
    "            critic_tW[iter] = tau * criticW[iter]+(1-tau)*critic_tW[iter]\n",
    "\n",
    "          actor_target.set_weights(actor_tW)\n",
    "          critic_target.set_weights(critic_tW)\n",
    "          \n",
    "          episode_policy_loss.append(policy_loss)\n",
    "          episode_critic_loss.append(critic_loss)         \n",
    "\n",
    "          if reward>0.9:\n",
    "            print(\"episode: {}, step: {}, reward: {}, average _reward: {} \\n, policy loss: {}, value loss: {}\".format(episode, step, episode_reward, np.mean(rewards[-10:]),policy_loss,critic_loss))\n",
    "            target_reached = True\n",
    "            steps =step\n",
    "            break\n",
    "          if done:\n",
    "            env.reset()\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    ep_av_policyloss.append(np.mean(episode_policy_loss))\n",
    "    ep_av_criticloss.append(np.mean(episode_critic_loss))\n",
    "    rewards.append(episode_reward)\n",
    "    steps_to_reward.append(steps)\n",
    "    \n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "    print(\"episode: {}, policy loss: {}, critic_loss: {}, reward_cum: {} \".format(episode, np.mean(episode_policy_loss),np.mean(episode_critic_loss), sum(rewards)))\n",
    "    #tot_loss.append(tot_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
