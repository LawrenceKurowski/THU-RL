{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MDP Value Iteration and Policy Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import * # imported environments from gym\n",
    "import time\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "    P: nested dictionary\n",
    "        From gym.core.Environment\n",
    "        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "        tuple of the form (probability, nextstate, reward, terminal) where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS: int\n",
    "        number of states in the environment\n",
    "    nA: int\n",
    "        number of actions in the environment\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "\"\"\"\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    V_new = value_function.copy()\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "  \n",
    "    # stopping condition\n",
    "    max_it = 1000\n",
    "    it = 0\n",
    "    \n",
    "    # iterate until convergence\n",
    "    while (it <= max_it or sum(np.abs(value_function - V_new))>tol):\n",
    "        value_function = V_new.copy()\n",
    "        # loop over all states\n",
    "        for s in range(nS):\n",
    "            # action at this state s according to current policy:\n",
    "            a = policy[s]\n",
    "            # load what happens from dictionary:\n",
    "            result = P[s][a]\n",
    "            # immediate reward (average over possible \"nextstates\"):\n",
    "            V_new[s] = np.array(result)[:,2].mean() \n",
    "        \n",
    "            # loop over possible \"nextstates\" (for stochastic case)\n",
    "            for index in range(len(result)):\n",
    "                # each scenario:\n",
    "                (p,s_prime,r,T) = result[index]\n",
    "                # add to get total value\n",
    "                V_new[s] += gamma * p * value_function[s_prime]\n",
    "        # update iteration:\n",
    "        it+=1\n",
    "        \n",
    "    return V_new\n",
    "   ############################\n",
    "\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    # define the Q function.\n",
    "    # greedy policy will pick a = argmax Q(s,a) for each s\n",
    "    Q = np.zeros([nS,nA])\n",
    "    \n",
    "    # loop over all states\n",
    "    for s in range(nS):\n",
    "        \n",
    "        # loop over all actions\n",
    "        for a in range(nA):\n",
    "            # result - load from dictionary\n",
    "            result = P[s][a]\n",
    "            \n",
    "            # expected value (loop over possible \"newstates\" in result)\n",
    "            for entry in range(len(result)):\n",
    "                p,s_prime,r,T = result[entry]\n",
    "                Q[s][a] = r\n",
    "                Q[s][a] += gamma * p * value_from_policy[s_prime]\n",
    "    \n",
    "    # we have a full Q matrix, for each state select a that maximises the reward:\n",
    "    new_policy = np.argmax(Q,axis=1)\n",
    "    \n",
    "    ############################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        tol parameter used in policy_evaluation()\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    # iteration stopping\n",
    "    max_iteration = 1000\n",
    "    it = 0 \n",
    "    \n",
    "    new_policy= policy.copy()\n",
    "    while (it <= max_iteration) or (sum(np.abs(new_policy - policy))>tol):\n",
    "        \n",
    "        # update policy\n",
    "        policy = new_policy\n",
    "        \n",
    "        # call policy_evaluation\n",
    "        # find value for the new policy\n",
    "        value_function = policy_evaluation(P, nS, nA, policy)\n",
    "        \n",
    "        # use this new value to update policy (greedy manner)\n",
    "        new_policy = policy_improvement(P, nS, nA, value_function, policy)\n",
    "        it += 1\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "    \n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    # iterations\n",
    "    idx = 0\n",
    "    max_iteration = 1000\n",
    "    \n",
    "    V_new = value_function.copy()\n",
    "    \n",
    "    while (idx <= max_iteration) or (sum(np.abs(V_new-value_function))>tol):\n",
    "  # loop over states\n",
    "        for s in range(nS):\n",
    "            # for comparing\n",
    "            max_result = -10\n",
    "            max_idx = 0\n",
    "            \n",
    "            # loop over actions\n",
    "            for a in range(nA):\n",
    "                result = P[s][a]\n",
    "                reward = np.array(result)[:,2].mean()\n",
    "                \n",
    "                # add expectation over possible s_prime\n",
    "                for entry in range(len(result)):\n",
    "                    (p, s_prime, r, T) = result[entry]\n",
    "                    reward += gamma*p*value_function[s_prime]\n",
    "                    \n",
    "                    # pick the reward > threshold\n",
    "                    if max_result < reward:\n",
    "                        \n",
    "                        max_result = reward\n",
    "                        max_idx = a\n",
    "            V_new[s] = max_result\n",
    "            policy[s] = max_idx\n",
    "\n",
    "        idx += 1\n",
    "        value_function = V_new\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "    \n",
    "    \n",
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "      print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "    env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    #env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    time_start = time.clock()\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "    V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_pi, 100)\n",
    "    \n",
    "    time_elapsed = (time.clock() - time_start)\n",
    "    print('Policy iteration computation time: '+str(time_elapsed))\n",
    "\n",
    "#*** \n",
    "\n",
    "    time_start = time.clock()\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_vi, 100)\n",
    "    \n",
    "    time_elapsed = (time.clock() - time_start)\n",
    "    print('Value iteration computation time: '+str(time_elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
