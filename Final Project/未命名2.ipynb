{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-ff760969b8ed>, line 220)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ff760969b8ed>\"\u001b[0;36m, line \u001b[0;32m220\u001b[0m\n\u001b[0;31m    i_episode += 1\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import gfootball.env as football_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import normalizer\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Google Football Q Learning')\n",
    "parser.add_argument('--lr', default=0.01, type=float)\n",
    "parser.add_argument('--actor_lr', default=0.0003, type=float)\n",
    "parser.add_argument('--critic_lr', default=0.0003, type=float)\n",
    "parser.add_argument('--eps', default=0.9, type=float)\n",
    "parser.add_argument('--gamma', default=0.1, type=float)\n",
    "parser.add_argument('--batch-size', default=128, type=int)\n",
    "parser.add_argument('--q-iteration', default=100, type=int)\n",
    "parser.add_argument('--env', default=0, type=int)\n",
    "parser.add_argument('--algo', default=1, type=int)\n",
    "parser.add_argument('--mem_cap', default=6000, type=int)\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "#Initialize Constants\n",
    "MEM_CAP = args.mem_cap #Lab GTX1080Tis have about 8 Gigs on board but we keep some buffer\n",
    "NUM_S = 115\n",
    "NUM_A = 19\n",
    "AUGMS = [\"none\",\"no_clipping\",\"no_reward_norm\",\"adam_al\",\"orth_init\"]\n",
    "SEEDS = [1,2,3,4,5,6,7,8,9,10]\n",
    "CHOSEN_AUGM = AUGMS[args.augm]\n",
    "CHOSEN_SEED = SEEDS[args.seed]\n",
    "print(f\"We are using augmentation {CHOSEN_AUGM}\")\n",
    "print(f\"We are using seed {CHOSEN_SEED}\")\n",
    "print(f\"We are using environment 11_vs_11_easy_stochastic\")\n",
    "print(f\"We are using algorithm PPO\")\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.net = PolicyNet()\n",
    "        self.optim = torch.optim.Adam(self.net.parameters(), lr=0.00008)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optim, NUM_S, eta_min=0, last_epoch=-1) ## added by Lawrence\n",
    "        self.normalizer = normalizer.RunningMeanStd()\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_counter = 0\n",
    "        self.memory_batch_counter = 0\n",
    "        self.memory = np.zeros((MEM_CAP, NUM_S + 5))\n",
    "        self.memory_batch = np.zeros((1, NUM_S * 2 + 4))\n",
    "        # Keep track of amount in memory to make sure there is no buffer overflow\n",
    "        # Memory: NEW_STATE, OLD_STATE, ACTION, REWARD, Policy Prob\n",
    "        self.values = torch.zeros(128).cuda()\n",
    "        self.selected_prob = torch.zeros(128).cuda() #Batch Size\n",
    "        #OpenAI Parameters\n",
    "        if CHOSEN_AUGM == \"no_clipping\":\n",
    "            self.epsilon = 0\n",
    "        else:\n",
    "            self.epsilon = 0.27\n",
    "        self.gamma = 0.993\n",
    "        self.lamb = 0.95\n",
    "        self.value_l_p = 0.5\n",
    "        self.entropy_p = 0.01\n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch from memory\n",
    "        sample_index = np.random.choice(MEM_CAP, args.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        states = torch.FloatTensor(batch_memory[:, :NUM_S]).cuda()\n",
    "        advantages = torch.FloatTensor(batch_memory[:, NUM_S:NUM_S + 1]).cuda()\n",
    "        rewards_to_go = torch.FloatTensor(batch_memory[:, NUM_S + 1:NUM_S + 2]).cuda()\n",
    "        values = torch.FloatTensor(batch_memory[:, NUM_S + 2: NUM_S + 3]).cuda()\n",
    "        actions = torch.LongTensor(batch_memory[:, NUM_S + 3: NUM_S + 4]).cuda()\n",
    "        selected_prob = torch.FloatTensor(batch_memory[:, NUM_S+4:NUM_S+5]).cuda()\n",
    "\n",
    "\n",
    "        values_new, dist_new = self.net(states)\n",
    "        values_new = values_new.flatten()\n",
    "        selected_prob_new = dist_new.log_prob(actions)\n",
    "\n",
    "        # Compute the PPO loss\n",
    "        prob_ratio = torch.exp(selected_prob_new) / torch.exp(selected_prob)\n",
    "\n",
    "        a = prob_ratio * advantages\n",
    "        b = torch.clamp(prob_ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "        ppo_loss = -1 * torch.mean(torch.min(a, b))\n",
    "\n",
    "        value_pred_clipped = values + (values_new - values).clamp(-self.epsilon, self.epsilon)\n",
    "        value_losses = (values_new - rewards_to_go) ** 2\n",
    "        value_losses_clipped = (value_pred_clipped - rewards_to_go) ** 2\n",
    "        value_loss = 0.5 * torch.max(value_losses, value_losses_clipped)\n",
    "        value_loss = value_loss.mean()\n",
    "        entropy_loss = torch.mean(dist_new.entropy())\n",
    "\n",
    "        loss = ppo_loss + self.value_l_p * value_loss - self.entropy_p * entropy_loss\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.net.parameters(), .5)\n",
    "        self.optim.step()\n",
    "        return float(loss)\n",
    "\n",
    "    def choose_action(self, s, shape_env_a):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0).cuda()\n",
    "\n",
    "        step_values, dist = self.net(s)\n",
    "\n",
    "        #Get actions from policy distribution\n",
    "        action = dist.sample()\n",
    "        policy_probs = dist.log_prob(action)\n",
    "\n",
    "        return action.cpu().tolist()[0], policy_probs ,step_values\n",
    "\n",
    "    def store_transition(self, s, a, r, s_plus_one, policy_probs, done, v):\n",
    "        # Store in memory, PPO also needs an advantage to be stored in memory\n",
    "\n",
    "        #Make a batch of not done and calculate advantages and normalized rewards when done\n",
    "        transition = np.hstack((s, [a, r], s_plus_one, policy_probs.cpu().detach(), [v.squeeze(0).cpu().detach()]))\n",
    "        self.memory_batch = np.vstack((self.memory_batch, transition))\n",
    "\n",
    "        # Handle normalization and Advantages\n",
    "        if done == 1:\n",
    "            values = self.memory_batch[1:, NUM_S + 3: NUM_S + 4]\n",
    "            rewards = self.memory_batch[1:, NUM_S + 1:NUM_S + 2]\n",
    "            rewards = self.reward_norm(rewards)\n",
    "            advantages = self.gae(rewards, values)\n",
    "            rewards_to_go = advantages + values\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
    "            actions = self.memory_batch[1:, NUM_S:NUM_S + 1].astype(int)\n",
    "            states = self.memory_batch[1:, :NUM_S]\n",
    "            policy_probs = self.memory_batch[1:, NUM_S + 3: NUM_S + 4]\n",
    "\n",
    "            ppo_needed_data = np.hstack((states,advantages, rewards_to_go, values, actions,  policy_probs))\n",
    "            index = self.memory_counter % MEM_CAP\n",
    "            if index + advantages.shape[0] <= MEM_CAP:\n",
    "                self.memory[index: index + advantages.shape[0], :] = ppo_needed_data\n",
    "            else:\n",
    "                self.memory[index:MEM_CAP, :] = ppo_needed_data[:MEM_CAP-index, :]\n",
    "\n",
    "            self.memory_batch = np.zeros((1, NUM_S * 2 + 4))\n",
    "            self.memory_counter += advantages.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def reward_norm(self, data, update_data=None, center=True,\n",
    "                         clip_limit=10):\n",
    "\n",
    "        \"\"\"Reward Normalization\"\"\"\n",
    "        if update_data is not None:\n",
    "            # Update the statistics with different data than we're normalizing\n",
    "            self.normalizer.update(update_data.reshape((-1,) + self.normalizer.shape))\n",
    "        else:\n",
    "            self.normalizer.update(data.reshape((-1,) + self.normalizer.shape))\n",
    "        if center:\n",
    "            data = data - self.normalizer.mean\n",
    "\n",
    "        if not CHOSEN_AUGM == \"no_reward_norm\":\n",
    "            data = data / np.sqrt(self.normalizer.var + 1e-8)\n",
    "        else:\n",
    "            continue\n",
    "        data = np.clip(data, -clip_limit, clip_limit)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def gae(self, rewards, values):\n",
    "        \"\"\"Generalized advantage estimate.\"\"\"\n",
    "        N = rewards.shape[0]  #Batch\n",
    "        T = rewards.shape[1]\n",
    "        gae_step = np.zeros((N,))\n",
    "        advantages = np.zeros((N, T))\n",
    "        for t in reversed(range(T - 1)):\n",
    "            one_step_td_error = rewards[:, t] + self.gamma * values[:, t + 1] - values[:, t]\n",
    "            current_gae_step = one_step_td_error + self.gamma * self.lamb * gae_step\n",
    "            advantages[:, t] = current_gae_step\n",
    "        return advantages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = football_env.create_environment(\n",
    "    env_name=\"11_vs_11_simple_stochastic\",\n",
    "    representation='simple115',\n",
    "    number_of_left_players_agent_controls=1,\n",
    "    stacked=False, logdir='/tmp/football',\n",
    "    write_goal_dumps=False,\n",
    "    write_full_episode_dumps=False,\n",
    "    render=False)\n",
    "\n",
    "\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample.shape\n",
    "\n",
    "max_episodes = 100000000 #Arbitrarily large number\n",
    "i_episode = 0\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "double = False\n",
    "dueling =False\n",
    "model = PPO()\n",
    "model.net.cuda()\n",
    "sum_sample_number = 0\n",
    "loss_sum = 0\n",
    "train_dict = {\"Episodes\": [], \"Loss\": [], \"Reward\": [], \"Step\": []}\n",
    "test_dict = {\"Episodes\": [], \"Reward\": []}\n",
    "loss_results = []\n",
    "total_done = False\n",
    "ep_reward_max = 0\n",
    "while i_episode < max_episodes:\n",
    "    \n",
    "    if CHOSEN_AUGM == \"adam_al\":\n",
    "        \n",
    "        scheduler.step()\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if i_episode % 100 is 0:\n",
    "        \n",
    "    i_episode += 1\n",
    "    env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    sample_number = 0\n",
    "    loss_sum = 0\n",
    "    while not done: #PPO also needs advantages computed here\n",
    "        sample_number += 1\n",
    "        sum_sample_number += 1\n",
    "        #\tprint(reward)\n",
    "        action = model.choose_action(state, ENV_A_SHAPE)\n",
    "        next_state, reward, done, infor = env.step(action)\n",
    "        model.store_transition(state, action, reward, next_state)\n",
    "        ep_reward += reward\n",
    "        if model.memory_counter >= MEM_CAP:\n",
    "            loss_sum += model.learn()\n",
    "            if done:\n",
    "                print(\n",
    "                    f\"Episode: {i_episode} Sample: {sample_number}, Episode Reward is {ep_reward}, Avg. Loss {loss_sum / sample_number}\")\n",
    "                train_dict[\"Episodes\"].append(i_episode)\n",
    "                train_dict[\"Loss\"].append(loss_sum)\n",
    "                train_dict[\"Reward\"].append(ep_reward)\n",
    "                train_dict[\"Step\"].append(sample_number)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if i_episode % 100 is 0:\n",
    "        print(\"Testing 10 Episodes...\")\n",
    "        ep_reward = 0\n",
    "        for i in range(10):\n",
    "            env.reset()\n",
    "            steps = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, policy_probs, step_values = model.choose_action(state, ENV_A_SHAPE)\n",
    "                next_state, reward, done, infor = env.step(action)\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        print(f\"[Test] Episode: {i_episode}  Avg. Episode Reward is {ep_reward / 10}\")\n",
    "        test_dict[\"Episodes\"].append(i_episode)\n",
    "        test_dict[\"Reward\"].append(ep_reward / 10)\n",
    "        if (ep_reward / 10 > ep_reward_max):\n",
    "            print(f\"New Best Avg Reward: {ep_reward / 10}\")\n",
    "            print(\"Saving Training Results...\")\n",
    "            pickle_train = open(f\"PPO_11_vs_11_simple_stochastic_train.p\", \"wb\")\n",
    "            pickle.dump(train_dict, pickle_train)\n",
    "            pickle_test = open(f\"PPO_11_vs_11_simple_stochastic_test.p\", \"wb\")\n",
    "            pickle.dump(test_dict, pickle_test)\n",
    "            ep_reward_max = (ep_reward / 10)\n",
    "\n",
    "        if (ep_reward / 10 == 1.):\n",
    "            total_done = True\n",
    "\n",
    "    state = next_state\n",
    "    if total_done:\n",
    "        print(\"Succesfully Trained!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
